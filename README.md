# rat-AI-touille
Cook book bot for you and me

Collection, filtering, preprocessing, tokenizing, encoding, embedding (with pretrained model)


Directory: 

    Collection: 
    - collection_script collecton script from spoon API
    - raw_recipes_0-5000 the raw data of recipes downloaded with all the information

    Processing:
    - preprocessing_general creating df with only necessary data as well as tokenizing, vectorizing, and embedding the data collected. Utilizing BertTokenizing model, a pretrained NLP to assist with embedding 
    - preprocesing1, filtering down necessary data

    Processed_data: 
    - different df's for models based off of necessary data, created in preprocessing files

    Tokenizing:
    - bert_tokenizing example for tokenizing data, utilized in preprocessing_general 

    To do:
    - run data through tokenizer and model, 
    - 

