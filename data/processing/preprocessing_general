import json
import pandas as pd
from transformers import BertTokenizer
from transformers import BertModel

file_path = "data/collection/raw_recipes_1250-2500.json"

def general_preprocess(file_path):
    with open(file_path, 'r') as file:
        data = json.load(file)
    recipes = pd.DataFrame(data)
    
    # remove all failures -- recipes without ids
    recipes = recipes.dropna(axis=0, subset=['id'])
    
    # Extract ingredient list and equipment list
    recipes['ingredient_list'] = recipes['analyzedInstructions'].map(lambda instruction: extract_ingred(instruction[0]) if len(instruction) > 0 else [])
    recipes['equipment_list'] = recipes['analyzedInstructions'].map(lambda instruction: extract_equip(instruction[0]) if len(instruction) > 0 else [])
    
    # Tokenize titles
    recipes['embed_title'] = recipes['title'].map(lambda title : embed(title))
    
    # Select only necessary columns
    recipes = recipes['image', 'title', 'embed_title', 'ingredient_list', 'equipment_list', 'dishTypes']
    
    # return recipes

general_preprocess(file_path)

# TODO: WRITE FINAL PREPROCESSED DATA TO JSON IF NECESSARY


# -------------- HELPER FUNCTIONS --------------

# Extract ingredient list from instructions
def extract_ingred(instruction):
    ingredients = []
    for step in instruction['steps']:
        if len(step['ingredients']) > 0:
            ingredients += [ingredient['name'] for ingredient in step['ingredients']]
    print(ingredients)
    return ingredients

# Extract equipment list from instructions
def extract_equip(instruction):
    equipment = []
    for step in instruction['steps']:
        if len(step['equipment']) > 0:
            equipment += [equip['name'] for equip in step['equipment']]
    print(equipment)
    return equipment


# Embeds strings using the bert tokenizer -- UNFINISHED (ola)
def embed(str):
    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    tokens = tokenizer.encode(str)